
\documentstyle[fp]{article}
\begin{document}

\newcommand{\hd}[1]{
\vspace*{0.2in} 
\noindent 
{\bf \Large #1}
\hspace{0.1in}
\vspace*{0.2in}}

%\vspace{0.5in}


\begin{Large}
\begin{center}
Teaching Statement \\
\hspace{1mm} \\
Kevin Knight \\
\hspace{1mm} \\
December 2011
\end{center}
\end{Large}

\vspace{0.1in}

\noindent
My teaching goals are:

\begin{itemize}
\item Explain hard concepts using common sense and examples.
\item Get new folks interested in joining my research field.
\item Empathize.
\end{itemize}

\noindent
The last goal means: the time for me to put together teaching 
materials is right after I've learned something difficult for myself.
I find that waiting too long means I
longer empathize enough with the first-time learner---so I try to 
seize those initial moments and write up teaching material.

\hd{Courses}

\noindent
For the last fifteen years, I have taught graduate courses in 
natural language processing at USC.  This has kept me connected with 
CS and EE graduate students.  My current course is organized around 
automata, probability, and linguistics. 
%My favorite part is teaching the EM learning algorithm.  
Here are my instructor ratings (1-5) from student evaluations for my
most recent courses: 4.77, 4.82, 4.73, 4.57, 4.64, 4.90, 4.92.

I also taught at the Stanford linguistics summer school 
(2007), UCLA extension (1996), and Beijing Mining College (1981).

I co-wrote the textbook {\em Artificial Intelligence, Second Edition} 
(1991) with Elaine Rich.  Because the field changed quite rapidly, 
we used very little material from the first edition.  This textbook 
was adopted for undergraduate and graduate courses throughout the world.

\hd{Tutorials}

\noindent
I regularly give conference tutorials on language translation and 
decipherment.  I have enjoyed doing 
tutorials by myself and jointly with colleagues Eduard Hovy, Philipp 
Koehn, David Chiang, and Richard Sproat.  I am currently working on 
an unsupervised learning tutorial.  (Unfortunately, ACL Executive Committee
meetings are held during ACL tutorial days, so my options have been
limited recently).

In addition to tutorials, I also value informative 
events such as the EM panel at the Empirical Methods in Natural 
Language Processing conference (2001), which I participated in 
jointly with Eugene Charniak, Stefan Riezler, and Ted Pedersen.

\hd{Workbooks}

\noindent
My most widely-read publications are not publications.  They are:

\begin{itemize}

\item {\em Statistical Machine Translation Tutorial Workbook} (1999). 
I created this workbook after spending substantial time with a
heavy seminal paper on statistical machine translation by Brown et
al (1993).  My goal 
was to present the material in a common-sense manner, with examples, 
and to communicate enthusiasm about a new scientific area.  This 
workbook has been the entry point for most people currently 
involved in language translation research.
I am very happy that my former PhD student Philipp Koehn has 
now published an entire textbook (2010) on the topic.

\item {\em Bayesian Inference with Tears:  A Tutorial Workbook for 
Natural Language Processing Researchers} (2009).  When I realized I was 
not understanding many of the talks presented at our conferences, I 
started reading up on Bayesian inference in natural language, and I found 
nothing introductory.
My goal for this workbook was to present the basics of 
Bayesian inference methods for structured-output problems like 
parsing and translation, using only eighth-grade math.  
I have received positive feedback about this workbook from many 
senior and junior researchers in the field.

\end{itemize}

\hd{Summer Internships}

\noindent
For the past decade, my colleagues and I have run a summer internship 
program in natural language processing at USC 
(nlg.isi.edu/get-involved/jobs.html).  Each year, we receive 80-100 
applications from students all over the world, and we select four.  
These students include both graduate students 
and undergraduates.  Many key inventions and publications have come 
out of these summer projects, and many undergraduates have decided to join 
our research field because of their positive experiences.  

The internship program has a heavy teaching component, as we give
intensive introductory tutorials on topics like
machine translation, unsupervised learning,
finite-state toolkits, and cluster computing.  Students return to
their home institutions with a solid grasp of these topics, as well
as research experience.

\end{document}
